{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Semi-Supervised Learning GANs.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"_eOvFumAKc5U","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import math"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dq9WAThjKc5Z","colab_type":"text"},"source":["### Load and Prepare Data"]},{"cell_type":"code","metadata":{"id":"ozOfjkJZKc5a","colab_type":"code","outputId":"5cc5ffc4-fead-4e00-9942-d7e59a5054b9","executionInfo":{"status":"ok","timestamp":1561281514290,"user_tz":-330,"elapsed":1450,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh4.googleusercontent.com/-83GaAJT3hHA/AAAAAAAAAAI/AAAAAAABMsk/e8STDzL1f1s/s64/photo.jpg","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["#Load MNIST data\n","(train_x, train_y),(test_x, test_y) = tf.keras.datasets.mnist.load_data()\n","\n","#Shape\n","train_x.shape"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(60000, 28, 28)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"G9lvlq81Kc5f","colab_type":"code","colab":{}},"source":["#Reshape images to be 3D\n","train_x = np.reshape(train_x, (-1,28,28,1))\n","test_x = np.reshape(test_x, (-1,28,28,1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FnFxPPobKc5i","colab_type":"code","outputId":"5014f125-3ade-4f55-c8ce-d52d61db0305","executionInfo":{"status":"ok","timestamp":1552197276374,"user_tz":-330,"elapsed":1222,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh4.googleusercontent.com/-83GaAJT3hHA/AAAAAAAAAAI/AAAAAAABMsk/e8STDzL1f1s/s64/photo.jpg","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_x.shape, test_x.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((60000, 28, 28, 1), (10000, 28, 28, 1))"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"91jqRH6MKc5l","colab_type":"code","colab":{}},"source":["#Normalize Data\n","train_x = train_x/127.5 - 1\n","test_x = test_x/127.5 - 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K2_WBVjoKc5o","colab_type":"text"},"source":["Split Training Data between Supervised and Unsupervised Examples. 10% of the data will be used in Supervised learning while rest of it will be used for UnSupervised Learning."]},{"cell_type":"code","metadata":{"id":"LeZpJ1-PKc5p","colab_type":"code","colab":{}},"source":["supervised_data_percent = 0.015\n","unsupervised_data_percent = 1 - supervised_data_percent"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmuMLj0VKc5s","colab_type":"code","colab":{}},"source":["train_x_sup, train_x_unsup, train_y_sup, train_y_unsup = train_test_split(train_x, train_y, \n","                                                                          train_size=supervised_data_percent,\n","                                                                          test_size=unsupervised_data_percent)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CaNuspBoKc5v","colab_type":"code","outputId":"2ff51d45-d401-44dc-f231-7c97e54bbadf","executionInfo":{"status":"ok","timestamp":1561281584326,"user_tz":-330,"elapsed":873,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh4.googleusercontent.com/-83GaAJT3hHA/AAAAAAAAAAI/AAAAAAABMsk/e8STDzL1f1s/s64/photo.jpg","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["train_x_sup.shape"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(900, 28, 28, 1)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"BY5BP2SOKc5z","colab_type":"text"},"source":["Following function will do 2 things:\n","\n","1. Convert MNIST labels to One-hot encoding\n","2. Append a column at the end with zeros to indicate Real Image"]},{"cell_type":"code","metadata":{"id":"m7xQ6D7vKc50","colab_type":"code","colab":{}},"source":["def prepare_labels(y):\n","    \n","    extended_labels = tf.keras.utils.to_categorical(y, 10)\n","    extended_labels = np.concatenate([extended_labels, np.zeros((extended_labels.shape[0],1))], axis=1)\n","    \n","    return extended_labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aKRx0iDWKc52","colab_type":"text"},"source":["### Build Generator\n","\n","Generator will take 100 random numbers as input and will produce an image of shape (28,28,1). Image data values will be between -1 to 1. "]},{"cell_type":"code","metadata":{"id":"kFTE3FD-Kc53","colab_type":"code","colab":{}},"source":["def generator(input_x, training, reuse=False):\n","    \n","    with tf.variable_scope('Generator', reuse=reuse) as scope:\n","        \n","        #Layer 0\n","        x = tf.keras.layers.Reshape((1,1,100,))(input_x)\n","        \n","        #Layer 1\n","        x = tf.keras.layers.Conv2DTranspose(100, kernel_size=(2,2), strides=1, padding='valid')(x)\n","        x = tf.layers.batch_normalization(x, training=training)\n","        x = tf.keras.activations.relu(x)\n","        \n","        #Layer 2\n","        x = tf.keras.layers.Conv2DTranspose(64, kernel_size=(3,3), strides=2, padding='valid')(x)\n","        x = tf.layers.batch_normalization(x, training=training)\n","        x = tf.keras.activations.relu(x)\n","        \n","        #Layer 3\n","        x = tf.keras.layers.Conv2DTranspose(32, kernel_size=(4,4), strides=2, padding='valid')(x)\n","        x = tf.layers.batch_normalization(x, training=training)\n","        x = tf.keras.activations.relu(x)\n","        \n","        #Layer 4\n","        x = tf.keras.layers.Conv2DTranspose(1, kernel_size=(6,6), strides=2, padding='valid')(x)\n","        x = tf.keras.activations.tanh(x)\n","        \n","        return x       "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KlEbaPeBKc55","colab_type":"text"},"source":["### Build Discriminator\n","\n","Discriminator will Images of shape (28,28,1) as input and will produce a vector with 11 values.\n","\n","- 10 Values for MNIST label Classification\n","- 1 Value for Classifying if image is Fake(1) OR Real(0)"]},{"cell_type":"code","metadata":{"id":"gwHFPECQKc56","colab_type":"code","colab":{}},"source":["def discriminator(input_d, p_drop, reuse=True, training = True):\n","    \n","    with tf.variable_scope('Discriminator', reuse=reuse) as scope:\n","        \n","        #Layer 1\n","        x = tf.keras.layers.Conv2D(32, kernel_size=(5,5), strides=2, padding='same')(input_d)\n","        x = tf.keras.layers.Dropout(p_drop)(x)\n","        x = tf.keras.activations.relu(x, alpha=0.2)\n","        \n","        #Layer 2\n","        x = tf.keras.layers.Conv2D(64, kernel_size=(3,3), strides=2, padding='same')(x)\n","        x = tf.layers.batch_normalization(x, training=training)\n","        x = tf.keras.activations.relu(x, alpha=0.2)\n","        \n","        #Layer 3\n","        x = tf.keras.layers.Conv2D(128, kernel_size=(2,2), strides=2, padding='same')(x)\n","        x = tf.layers.batch_normalization(x, training=training)\n","        x = tf.keras.activations.relu(x, alpha=0.2)\n","        x = tf.keras.layers.Dropout(p_drop)(x)\n","        \n","        #Layer 4\n","        x = tf.keras.layers.Conv2D(128, kernel_size=(2,2), strides=2, padding='same')(x)\n","        x = tf.keras.activations.relu(x, alpha=0.2)\n","        \n","        #Layer 5\n","        features = tf.keras.layers.Flatten()(x)\n","        logits = tf.keras.layers.Dense(11)(features)\n","        \n","        return features, logits"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"erUnJ1DMKc58","colab_type":"text"},"source":["### Define Loss\n","\n","Loss will be calculated for the following 3 inputs:\n","\n","1. Real images with actual labels (Supervised Learning)\n","2. Real images with NO labels (Unsupervised Learning)\n","3. Fake images with NO labels (Unsupervised Learning)\n","\n","\n","Loss will be calculated for Discriminator and Generator. \n","\n","#### 1. Discriminator Loss\n","\n","Following will be considered to calculate Loss:\n","\n","Unsupervised:\n","1. Loss to predict Real Image is Real and Not fake.\n","2. Loss to predict Fake Image is Fake and Not Real.\n","\n","Supervised:\n","1. Loss to predict MNIST label classification\n","\n","#### 2. Generator Loss\n","\n","Unsupervised Loss:\n","1. Loss to predict Fake Image as Real\n","2. Feature Mapping loss "]},{"cell_type":"code","metadata":{"id":"d0iGY1K1Kc59","colab_type":"code","colab":{}},"source":["def model_loss(real_un_sup_ip, real_sup_ip, fake_ip, p_drop, training, y):\n","    \n","        \n","    #Get Discriminator output for Real Supervised Data\n","    rs_features, rs_logits = discriminator(real_sup_ip, p_drop, reuse=False, training=training)\n","    \n","    #Get Discriminator output for Real Un-Supervised Data\n","    ru_features, ru_logits = discriminator(real_un_sup_ip, p_drop, reuse=True, training=training)\n","    \n","    #Get Fake images from Generator\n","    fake_images = generator(fake_ip, training=training)\n","    \n","    #Get Dicriminator output for Fake images\n","    fake_features, fake_logits = discriminator(fake_images, p_drop, reuse=True, training=training)\n","    \n","    \n","    #Calculating Discriminator Loss\n","    \n","    #1. Let's calculate Unsupervised Loss for both Real and Fake data\n","    real_un_sup_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=ru_logits[:,-1], \n","                                                                              labels=tf.zeros_like(ru_logits[:,-1])))\n","        \n","    \n","    fake_un_sup_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits[:,-1], \n","                                                                              labels=tf.ones_like(fake_logits[:,-1])))\n","    \n","    #2. Supervised Loss\n","    real_sup_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=rs_logits, \n","                                                                              labels=y))\n","        \n","    d_loss = real_un_sup_loss + fake_un_sup_loss + real_sup_loss\n","    \n","    \n","    #Calculating feature mapping loss for Generator\n","    tmp1 = tf.reduce_mean(ru_features, axis = 0)\n","    tmp2 = tf.reduce_mean(fake_features, axis = 0)\n","    feature_mapping_loss = tf.reduce_mean(tf.square(tmp1 - tmp2))\n","    \n","    #Fake vs Real loss\n","    fake_loss_2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits[:,-1], \n","                                                                              labels=tf.zeros_like(fake_logits[:,-1])))\n","    \n","    #g_loss = feature_mapping_loss +  fake_loss_2\n","    g_loss = fake_loss_2\n","    \n","    rs_class_op = tf.nn.softmax(rs_logits)\n","    \n","    #Calculate Accuracy\n","    correct_prediction = tf.equal(tf.argmax(rs_class_op, axis=1), tf.argmax(y, axis=1))\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","    \n","    return fake_images, d_loss, g_loss, accuracy, rs_logits"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TuiUihpVKc6A","colab_type":"text"},"source":["### Model Optimization\n","\n","Training Discriminator and Generator models"]},{"cell_type":"code","metadata":{"id":"qX4DaTDFKc6B","colab_type":"code","colab":{}},"source":["def model_optimization(d_loss, g_loss):\n","    \n","    # Get weights and biases to update. Get them separately for the discriminator and the generator\n","    discriminator_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES , scope='Discriminator')    \n","    generator_train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Generator')\n","    \n","    #Minimize loss\n","    d_opt = tf.train.AdamOptimizer(name='d_optimizer').minimize(d_loss, var_list=discriminator_train_vars)\n","    \n","    g_opt = tf.train.AdamOptimizer(name='g_optimizer').minimize(g_loss, var_list=generator_train_vars)\n","    \n","    return d_opt, g_opt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gzJKcBhdKc6D","colab_type":"text"},"source":["### Training Module"]},{"cell_type":"code","metadata":{"id":"AJZZ74LmKc6E","colab_type":"code","colab":{}},"source":["def train(batch_size = 64, epochs = 1000):\n","    \n","    train_D_losses = []\n","    train_G_losses = []\n","    train_Accs  = []\n","    test_D_losses = []\n","    test_G_losses = []\n","    test_Accs = []\n","    noise_size = 100\n","    \n","    \n","    tf.reset_default_graph()\n","    \n","    #Declare Placeholders for input values\n","    real_sup_img = tf.placeholder(tf.float32, shape=(None,28,28,1))\n","    labels = tf.placeholder(tf.int64, shape=(None))\n","    \n","    real_unsup_img = tf.placeholder(tf.float32, shape=(None,28,28,1))\n","    \n","    noise_input = tf.placeholder(tf.float32, shape=(None, noise_size))\n","    \n","    dropout_rate = tf.placeholder(tf.float32)\n","    training = tf.placeholder(tf.bool)\n","    \n","    #Learning rate for Generator and Discriminator\n","    lr_g = tf.placeholder(tf.float32)\n","    lr_d = tf.placeholder(tf.float32)\n","    \n","    \n","    #Build the Graph\n","    fake_images, d_loss, g_loss, accuracy, dis_op = model_loss(real_unsup_img, real_sup_img, noise_input, dropout_rate, \n","                                                       training, labels)    \n","    d_opt, g_opt = model_optimization(d_loss, g_loss)\n","    \n","    \n","    #Execute Graph\n","    with tf.Session() as sess:\n","        \n","        sess.run(tf.global_variables_initializer())\n","        \n","        for i in range(epochs):\n","            \n","            #90% real images will be unsupervised\n","            unsup_indexes = np.random.randint(0, train_x_unsup.shape[0], size=int(0.9*batch_size))\n","            #10% of images will be supervised\n","            sup_indexes = np.random.randint(0, train_x_sup.shape[0], size=int(0.1*batch_size))\n","            \n","            \n","            train_feed_dict = {real_sup_img: train_x_sup[sup_indexes], \n","                         labels: prepare_labels(train_y_sup[sup_indexes]), \n","                         real_unsup_img: train_x_unsup[unsup_indexes], \n","                         noise_input: np.random.uniform(-1.0, 1.0, size = (batch_size, 100)), \n","                         dropout_rate: 0.5,\n","                         training: True,\n","                         lr_g: 1e-5, \n","                         lr_d: 1e-5}\n","            \n","            _,_, dloss, gloss, acc = sess.run([d_opt, g_opt, d_loss, g_loss, accuracy], feed_dict=train_feed_dict)\n","            \n","            \n","            #Calculate Loss and Accuracy for Test Data\n","            if i % 200 == 0:\n","                \n","                print(i, '. Training Acc', acc, end='\\t')\n","                train_Accs.append(acc)\n","                \n","                test_feed_dict = {real_sup_img: test_x, \n","                         labels: prepare_labels(test_y), \n","                         real_unsup_img: test_x, \n","                         noise_input: np.random.uniform(-1.0, 1.0, size = (batch_size, 100)), \n","                         dropout_rate: 0,\n","                         training: False}\n","                \n","                t_dloss, t_gloss, t_acc, fakeImgs = sess.run([d_loss, g_loss, accuracy, fake_images], \n","                                                             feed_dict=test_feed_dict)\n","                \n","                test_Accs.append(t_acc)\n","                \n","                print('Test Acc', t_acc)\n","    return train_Accs, test_Accs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b8vrkzUMKc6H","colab_type":"code","outputId":"0b073f2c-6c8c-46a0-daf0-cce64ead13c1","colab":{}},"source":["accs, val_accs = train(batch_size=32,epochs=20000)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 . Training Acc 0.0\tTest Acc 0.1367\n","200 . Training Acc 1.0\tTest Acc 0.7714\n","400 . Training Acc 1.0\tTest Acc 0.8044\n","600 . Training Acc 0.6666667\tTest Acc 0.8624\n","800 . Training Acc 1.0\tTest Acc 0.846\n","1000 . Training Acc 1.0\tTest Acc 0.8885\n","1200 . Training Acc 1.0\tTest Acc 0.8819\n","1400 . Training Acc 1.0\tTest Acc 0.8665\n","1600 . Training Acc 1.0\tTest Acc 0.8998\n","1800 . Training Acc 1.0\tTest Acc 0.6719\n","2000 . Training Acc 1.0\tTest Acc 0.9109\n","2200 . Training Acc 1.0\tTest Acc 0.91\n","2400 . Training Acc 1.0\tTest Acc 0.9168\n","2600 . Training Acc 0.6666667\tTest Acc 0.9029\n","2800 . Training Acc 1.0\tTest Acc 0.9347\n","3000 . Training Acc 1.0\tTest Acc 0.9166\n","3200 . Training Acc 1.0\tTest Acc 0.9209\n","3400 . Training Acc 1.0\tTest Acc 0.9111\n","3600 . Training Acc 1.0\tTest Acc 0.8787\n","3800 . Training Acc 0.6666667\tTest Acc 0.8759\n","4000 . Training Acc 0.6666667\tTest Acc 0.6957\n","4200 . Training Acc 1.0\tTest Acc 0.9185\n","4400 . Training Acc 1.0\tTest Acc 0.9078\n","4600 . Training Acc 1.0\tTest Acc 0.9108\n","4800 . Training Acc 1.0\tTest Acc 0.8917\n","5000 . Training Acc 1.0\tTest Acc 0.843\n","5200 . Training Acc 1.0\tTest Acc 0.9015\n","5400 . Training Acc 1.0\tTest Acc 0.9268\n","5600 . Training Acc 0.6666667\tTest Acc 0.8939\n","5800 . Training Acc 1.0\tTest Acc 0.8929\n","6000 . Training Acc 1.0\tTest Acc 0.8727\n","6200 . Training Acc 1.0\tTest Acc 0.9046\n","6400 . Training Acc 1.0\tTest Acc 0.8978\n","6600 . Training Acc 1.0\tTest Acc 0.9338\n","6800 . Training Acc 1.0\tTest Acc 0.9043\n","7000 . Training Acc 1.0\tTest Acc 0.9247\n","7200 . Training Acc 1.0\tTest Acc 0.9311\n","7400 . Training Acc 1.0\tTest Acc 0.9345\n","7600 . Training Acc 1.0\tTest Acc 0.936\n","7800 . Training Acc 1.0\tTest Acc 0.9339\n","8000 . Training Acc 1.0\tTest Acc 0.9344\n","8200 . Training Acc 1.0\tTest Acc 0.9362\n","8400 . Training Acc 1.0\tTest Acc 0.9366\n","8600 . Training Acc 1.0\tTest Acc 0.9362\n","8800 . Training Acc 1.0\tTest Acc 0.9371\n","9000 . Training Acc 1.0\tTest Acc 0.9375\n","9200 . Training Acc 1.0\tTest Acc 0.9377\n","9400 . Training Acc 1.0\tTest Acc 0.9377\n","9600 . Training Acc 1.0\tTest Acc 0.9382\n","9800 . Training Acc 1.0\tTest Acc 0.9382\n","10000 . Training Acc 1.0\tTest Acc 0.9389\n","10200 . Training Acc 1.0\tTest Acc 0.9394\n","10400 . Training Acc 1.0\tTest Acc 0.9397\n","10600 . Training Acc 1.0\tTest Acc 0.9396\n","10800 . Training Acc 1.0\tTest Acc 0.9401\n","11000 . Training Acc 1.0\tTest Acc 0.9402\n","11200 . Training Acc 1.0\tTest Acc 0.9407\n","11400 . Training Acc 1.0\tTest Acc 0.9411\n","11600 . Training Acc 1.0\tTest Acc 0.9415\n","11800 . Training Acc 1.0\tTest Acc 0.9417\n","12000 . Training Acc 1.0\tTest Acc 0.9414\n","12200 . Training Acc 1.0\tTest Acc 0.941\n","12400 . Training Acc 1.0\tTest Acc 0.9419\n","12600 . Training Acc 1.0\tTest Acc 0.9426\n","12800 . Training Acc 1.0\tTest Acc 0.9418\n","13000 . Training Acc 1.0\tTest Acc 0.9421\n","13200 . Training Acc 1.0\tTest Acc 0.9421\n","13400 . Training Acc 1.0\tTest Acc 0.9421\n","13600 . Training Acc 1.0\tTest Acc 0.9434\n","13800 . Training Acc 1.0\tTest Acc 0.938\n","14000 . Training Acc 1.0\tTest Acc 0.9399\n","14200 . Training Acc 1.0\tTest Acc 0.8465\n","14400 . Training Acc 0.6666667\tTest Acc 0.8949\n","14600 . Training Acc 1.0\tTest Acc 0.9148\n","14800 . Training Acc 1.0\tTest Acc 0.9186\n","15000 . Training Acc 1.0\tTest Acc 0.9159\n","15200 . Training Acc 1.0\tTest Acc 0.9206\n","15400 . Training Acc 1.0\tTest Acc 0.9236\n","15600 . Training Acc 1.0\tTest Acc 0.9211\n","15800 . Training Acc 1.0\tTest Acc 0.9198\n","16000 . Training Acc 1.0\tTest Acc 0.9136\n","16200 . Training Acc 1.0\tTest Acc 0.9134\n","16400 . Training Acc 1.0\tTest Acc 0.911\n","16600 . Training Acc 1.0\tTest Acc 0.9223\n","16800 . Training Acc 1.0\tTest Acc 0.8954\n","17000 . Training Acc 1.0\tTest Acc 0.901\n","17200 . Training Acc 1.0\tTest Acc 0.8993\n","17400 . Training Acc 1.0\tTest Acc 0.9215\n","17600 . Training Acc 1.0\tTest Acc 0.9175\n","17800 . Training Acc 1.0\tTest Acc 0.9261\n","18000 . Training Acc 1.0\tTest Acc 0.9185\n","18200 . Training Acc 1.0\tTest Acc 0.931\n","18400 . Training Acc 1.0\tTest Acc 0.895\n","18600 . Training Acc 1.0\tTest Acc 0.9029\n","18800 . Training Acc 1.0\tTest Acc 0.9146\n","19000 . Training Acc 1.0\tTest Acc 0.9305\n","19200 . Training Acc 1.0\tTest Acc 0.9318\n","19400 . Training Acc 1.0\tTest Acc 0.9336\n","19600 . Training Acc 1.0\tTest Acc 0.9321\n","19800 . Training Acc 1.0\tTest Acc 0.9306\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EXDLwpCqKc6M","colab_type":"code","colab":{}},"source":["def plot_images(fake_images):\n","    \n","    plt.figure(figsize=(2.2, 2.2))\n","    num_images = 16\n","    \n","    image_size = 28\n","    rows = 4\n","    \n","    for i in range(num_images):\n","        plt.subplot(rows, rows, i + 1)\n","        image = np.reshape(fake_images[i], [image_size, image_size])\n","        image = (image + 1)/2\n","        plt.imshow(image, cmap='gray')\n","        plt.axis('off')\n","    plt.show()   "],"execution_count":0,"outputs":[]}]}