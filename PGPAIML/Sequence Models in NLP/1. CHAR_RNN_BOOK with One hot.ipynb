{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjMJKaBXi9ru",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NdRMc0S0qZlv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjMJKaBXi9ru",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collect Data\n",
    "<font size=\"2\">Download data from Project Gutenberg site -> http://www.gutenberg.org/files/1342/1342-0.txt</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://www.gutenberg.org/files/1342/1342-0.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "USE1lx0aqZl3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the book:  704190\n"
     ]
    }
   ],
   "source": [
    "book_text = open('1342-0.txt', encoding='utf8').read() #reading the book as a string\n",
    "print('Length of the book: ' , len(book_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "oZcO2q06yVDr",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ful property\\nof some one or other of their daughters.\\n\\n“My dear Mr. Bennet,” said his lady to him one day, “have you heard that\\nNetherfield Park is let at last?”\\n\\nMr. Bennet replied that he had not.\\n\\n“But it is,” returned she; “for Mrs. Long has just been here, and she\\ntold me all about it.”\\n\\nMr. Bennet made no answer.\\n\\n“Do you not want to know who has taken it?” cried his wife impatiently.\\n\\n“_You_ want to tell me, and I have no objection to hearing it.”\\n\\nThis was invitation enough.\\n\\n“Why, my de'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_text[1000:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jT0ptJlHqZmA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WqoU8rqQqZmB",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters:  86\n"
     ]
    }
   ],
   "source": [
    "#Tokenize at character level\n",
    "t = tf.keras.preprocessing.text.Tokenizer(char_level=True, lower=False)\n",
    "\n",
    "#Fit tokenizer on the book\n",
    "t.fit_on_texts(book_text)\n",
    "\n",
    "#Vocablury size\n",
    "vocab_size = len(t.word_index)\n",
    "\n",
    "print('Number of unique characters: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, 'e': 2, 't': 3, 'a': 4, 'o': 5, 'n': 6, 'i': 7, 'h': 8, 'r': 9, 's': 10, 'd': 11, 'l': 12, 'u': 13, '\\n': 14, 'm': 15, 'c': 16, 'y': 17, 'f': 18, 'w': 19, 'g': 20, ',': 21, 'p': 22, 'b': 23, '.': 24, 'v': 25, 'k': 26, 'I': 27, '“': 28, '”': 29, 'M': 30, ';': 31, '-': 32, 'B': 33, 'z': 34, 'T': 35, 'x': 36, 'E': 37, '_': 38, 'L': 39, \"'\": 40, 'H': 41, 'C': 42, 'W': 43, 'j': 44, 'q': 45, 'D': 46, 'S': 47, 'A': 48, '!': 49, '?': 50, 'Y': 51, 'J': 52, 'P': 53, 'N': 54, 'G': 55, 'O': 56, 'F': 57, 'R': 58, ':': 59, 'K': 60, '1': 61, 'U': 62, '*': 63, '(': 64, ')': 65, '2': 66, '3': 67, '4': 68, '0': 69, 'V': 70, '5': 71, '/': 72, '8': 73, '6': 74, '9': 75, '7': 76, 'Z': 77, 'X': 78, '@': 79, '$': 80, '\\ufeff': 81, '[': 82, '#': 83, ']': 84, '%': 85, 'Q': 86}\n"
     ]
    }
   ],
   "source": [
    "#Character Vocabluty\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18], [13], [12], [1], [22], [9], [5], [22], [2], [9], [3], [17], [14], [5], [18], [1], [10], [5], [15], [2], [1], [5], [6], [2], [1], [5], [9], [1], [5], [3], [8], [2], [9], [1], [5], [18], [1], [3], [8], [2], [7], [9], [1], [11], [4], [13], [20], [8], [3], [2]]\n"
     ]
    }
   ],
   "source": [
    "#Convert characters in the book to Numbers\n",
    "book_num = t.texts_to_sequences(book_text)\n",
    "print(book_num[1000:1050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fm0pgAzaIa72",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Build a dictionary which can convert numbers into chars\n",
    "int_to_char = dict((i,c) for c, i in t.word_index.items()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_char[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhTY6QPdqZmY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prepare Input and Output Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1IGYYK6ZqZmd",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Input and output container\n",
    "- Input data will have sequences with 100 characters\n",
    "- Output data will have one character which comes after 100 characters in the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XfgONYykqZme",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sequence_length = 100 #Length of input sequence\n",
    "\n",
    "#Empty list for input and output data\n",
    "input_data = []  #Empty list for input data\n",
    "output_data = [] #Empty list for output data\n",
    "\n",
    "#Populate input and output data\n",
    "for i in range(0, len(book_num) - sequence_length):\n",
    "    \n",
    "    input_seq = book_num[i : i + sequence_length] #input sequence    \n",
    "    output_seq = book_num[i + sequence_length] #Output sequence\n",
    "    \n",
    "    input_data.append(input_seq)\n",
    "    output_data.append(output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DbMa8MC60lHo",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of input arrays:  704090\n",
      "Total number of Output arrays:  704090\n",
      "Input Data length:  100\n",
      "Output Data length:  1\n"
     ]
    }
   ],
   "source": [
    "print('Total number of input arrays: ', len(input_data))\n",
    "print('Total number of Output arrays: ', len(output_data))\n",
    "print(\"Input Data length: \",len(input_data[10]))\n",
    "print(\"Output Data length: \",len(output_data[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bool_example = 'I am home'\n",
    "example_size = 5  #Num of chars in each example\n",
    "1 Example = 'I am '\n",
    "2nd - ' am h'\n",
    "3rd - 'am ho'\n",
    "4th = 'm hom'\n",
    "5 - ' home'\n",
    "\n",
    "finally 5 examples\n",
    "9 - 5 + 1\n",
    "\n",
    "704190 - 100 = 704090 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16],\n",
       " [3],\n",
       " [1],\n",
       " [55],\n",
       " [13],\n",
       " [3],\n",
       " [2],\n",
       " [6],\n",
       " [23],\n",
       " [2],\n",
       " [9],\n",
       " [20],\n",
       " [1],\n",
       " [37],\n",
       " [33],\n",
       " [5],\n",
       " [5],\n",
       " [26],\n",
       " [1],\n",
       " [5],\n",
       " [18],\n",
       " [1],\n",
       " [53],\n",
       " [9],\n",
       " [7],\n",
       " [11],\n",
       " [2],\n",
       " [1],\n",
       " [4],\n",
       " [6],\n",
       " [11],\n",
       " [1],\n",
       " [53],\n",
       " [9],\n",
       " [2],\n",
       " [44],\n",
       " [13],\n",
       " [11],\n",
       " [7],\n",
       " [16],\n",
       " [2],\n",
       " [21],\n",
       " [1],\n",
       " [23],\n",
       " [17],\n",
       " [1],\n",
       " [52],\n",
       " [4],\n",
       " [6],\n",
       " [2],\n",
       " [1],\n",
       " [48],\n",
       " [13],\n",
       " [10],\n",
       " [3],\n",
       " [2],\n",
       " [6],\n",
       " [14],\n",
       " [14],\n",
       " [35],\n",
       " [8],\n",
       " [7],\n",
       " [10],\n",
       " [1],\n",
       " [2],\n",
       " [33],\n",
       " [5],\n",
       " [5],\n",
       " [26],\n",
       " [1],\n",
       " [7],\n",
       " [10],\n",
       " [1],\n",
       " [18],\n",
       " [5],\n",
       " [9],\n",
       " [1],\n",
       " [3],\n",
       " [8],\n",
       " [2],\n",
       " [1],\n",
       " [13],\n",
       " [10],\n",
       " [2],\n",
       " [1],\n",
       " [5],\n",
       " [18],\n",
       " [1],\n",
       " [4],\n",
       " [6],\n",
       " [17],\n",
       " [5],\n",
       " [6],\n",
       " [2],\n",
       " [1],\n",
       " [4],\n",
       " [6],\n",
       " [17],\n",
       " [19],\n",
       " [8]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QLLac8rUqZmp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One Hot encoding for Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "704090 * 1 * 87 * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_data = 704090 x 100 x 87 = 6,125,583,000\n",
    "Each integer = 4 Bytes\n",
    "Number of Bytes = 24,502,332,000 = 24.5 Giga Bytes\n",
    "Output = 245MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tfvxJwgSZuLs",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b292dadcfe05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Input data one hot encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minput_data_one_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Output data one hot encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moutput_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rajeev\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m   \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m   \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m   \u001b[0mcategorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m   \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Input data one hot encoding\n",
    "input_data_one_hot = tf.keras.utils.to_categorical(input_data,num_classes=vocab_size+1)\n",
    "\n",
    "#Output data one hot encoding\n",
    "output_data = tf.keras.utils.to_categorical(output_data,num_classes=vocab_size+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input = 700K+\n",
    "- Each Array = 100 Chracter\n",
    "- 700K*100 = 70M Numbers\n",
    "- 1 Character = 86 numbers\n",
    "- Total Numbers - 700K * 100 * 86 = 6,020,000,000\n",
    "1 integer = 4 Byters\n",
    "6 B * 4 = 24B Byters = 24 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2000 * 100 * 60 * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QLLac8rUqZmp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reshaping input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "f8E7hXgQ6KSV",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Reshape input data into 3 dimensional numpy array\n",
    "#batch_size x sequence_length x vocab_size+1\n",
    "input_data_one_hot = np.reshape(input_data_one_hot,\n",
    "                                (len(input_data_one_hot),\n",
    "                                 sequence_length,\n",
    "                                 vocab_size+1))\n",
    "\n",
    "print(input_data_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "831Cc5dkqZm7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5FmHZZrnqZm8"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(256, input_shape=(input_data.shape[1],input_data.shape[2])))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(vocab_size+1, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy') #No accuracy tracking here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlmHfhSUqZnW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lyh80KAdpEXB"
   },
   "outputs": [],
   "source": [
    "#Identify a random sequence which we will use to generate output\n",
    "test_seq =  input_data[np.random.randint(0, high=input_data.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ste0lSBs-igA"
   },
   "outputs": [],
   "source": [
    "def predict_seq(epoch, logs):\n",
    "    \n",
    "    print('Output sequence is: ')\n",
    "    \n",
    "    #Initialize predicted output\n",
    "    predicted_output = ''\n",
    "    \n",
    "    #lets predict 50 next chars\n",
    "    current_seq = np.copy(test_seq)\n",
    "    for i in range(50):\n",
    "        current_seq_one_hot = to_categorical(current_seq, num_classes=vocab_size+1)\n",
    "        data_input = np.reshape(current_seq_one_hot,(1,\n",
    "                                                     current_seq_one_hot.shape[0],\n",
    "                                                     current_seq_one_hot.shape[1]))\n",
    "        \n",
    "        #Get the char int with maximum probability\n",
    "        predicted_char_int = np.argmax(model.predict(data_input)[0])\n",
    "        \n",
    "        #Add to the predicted out, convert int to char\n",
    "        predicted_output = predicted_output + int_to_char[predicted_char_int]\n",
    "        \n",
    "        #Update seq with new value at the end\n",
    "        current_seq = np.roll(current_seq, -1)\n",
    "        current_seq[current_seq.shape[0]-1] = [predicted_char_int]\n",
    "    \n",
    "    print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VW72ruTt1I9r"
   },
   "outputs": [],
   "source": [
    "#Create a LabdaCallback to do prediction at end of every epoch\n",
    "checkpoint = LambdaCallback(on_epoch_end=predict_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fObNxWaeLdg5"
   },
   "outputs": [],
   "source": [
    "#Print random starting sequence for prediction\n",
    "print('Initial sequence is: ')\n",
    "for i in range (sequence_length):\n",
    "    print(int_to_char[int(test_seq[i]*vocab_size)], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HAVH-aqyqZna"
   },
   "outputs": [],
   "source": [
    "model.fit(input_data, output_data, \n",
    "          batch_size=128, \n",
    "          epochs=50,\n",
    "          callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "2a. CHAR_RNN_BOOK.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
