{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjMJKaBXi9ru",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NdRMc0S0qZlv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjMJKaBXi9ru",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collect Data\n",
    "<font size=\"2\">Download data from Project Gutenberg site -> http://www.gutenberg.org/files/1342/1342-0.txt </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "USE1lx0aqZl3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the book:  704190\n"
     ]
    }
   ],
   "source": [
    "book_text = open('1342-0.txt', encoding='utf8').read()\n",
    "print('Length of the book: ' , len(book_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "oZcO2q06yVDr",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ful property\\nof some one or other of their daughters.\\n\\n“My dear Mr. Bennet,” said his lady to him on'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_text[1000:1100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jT0ptJlHqZmA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WqoU8rqQqZmB",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters:  60\n"
     ]
    }
   ],
   "source": [
    "#Tokenize at character level\n",
    "t = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "\n",
    "#Fit tokenizer on the book\n",
    "t.fit_on_texts(book_text)\n",
    "\n",
    "#Vocablury size\n",
    "vocab_size = len(t.word_index)\n",
    "print('Number of unique characters: ', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Convert characters in the book to Numbers\n",
    "book_num = t.texts_to_sequences(book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Build a dictionary which can convert numbers into chars\n",
    "int_to_char = dict((i,c) for c, i in t.word_index.items()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fhTY6QPdqZmY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "text",
    "id": "1IGYYK6ZqZmd",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "record_num = 0 #starting batch number\n",
    "sequence_length = 100 #Length of input sequence\n",
    "\n",
    "def batch_generator(batch_size=32):\n",
    "    \n",
    "    #Will update batch_num\n",
    "    global record_num    \n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        #Empty list for input and output data\n",
    "        input_data = []\n",
    "        output_data = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            #input sequence\n",
    "            input_seq = book_num[(record_num * sequence_length) : (record_num * sequence_length) + sequence_length]\n",
    "            #Output sequence\n",
    "            output_seq = book_num[(record_num * sequence_length) + sequence_length]\n",
    "\n",
    "            input_data.append(input_seq)\n",
    "            output_data.append(output_seq)\n",
    "            \n",
    "            record_num = record_num + 1\n",
    "            \n",
    "            if((record_num*sequence_length + sequence_length) > len(book_num)):\n",
    "                record_num = 0\n",
    "                \n",
    "\n",
    "        #Input data one hot encoding\n",
    "        input_data = tf.keras.utils.to_categorical(input_data,num_classes=vocab_size+1)\n",
    "\n",
    "        #Output data one hot encoding\n",
    "        output_data = tf.keras.utils.to_categorical(output_data,num_classes=vocab_size+1)\n",
    "\n",
    "        #Reshape input data into 3 dimensional numpy array\n",
    "        #batch_size x sequence_length x vocab_size+1\n",
    "        input_data = np.reshape(input_data,\n",
    "                                (len(input_data),\n",
    "                                 sequence_length,\n",
    "                                 vocab_size+1))\n",
    "        \n",
    "        yield input_data, output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "831Cc5dkqZm7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5FmHZZrnqZm8"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#LSTM\n",
    "model.add(tf.keras.layers.LSTM(256, input_shape=(sequence_length,vocab_size+1)))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(vocab_size+1, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy') #No accuracy tracking here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlmHfhSUqZnW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Print model output during Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lyh80KAdpEXB",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Identify a random sequence which we will use to generate output\n",
    "start_pos = np.random.randint(0, high=(len(book_num) - sequence_length))\n",
    "test_seq =  book_num[start_pos : start_pos+sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fObNxWaeLdg5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial sequence is: \n",
      "nd censure of the ladies both of netherfield and\n",
      "rosings.\n",
      "\n",
      "their visitors stayed with them above hal"
     ]
    }
   ],
   "source": [
    "#Print random starting sequence for prediction\n",
    "print('Initial sequence is: ')\n",
    "for i in range (sequence_length):\n",
    "    print(int_to_char[test_seq[i][0]], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlmHfhSUqZnW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ste0lSBs-igA",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def predict_seq(epoch, logs):\n",
    "    \n",
    "    print('\\n\\nOutput sequence after epoch ', epoch, ' :')\n",
    "    \n",
    "    #Initialize predicted output\n",
    "    predicted_output = ''\n",
    "    \n",
    "    #lets predict 50 next chars\n",
    "    current_seq = np.copy(test_seq)\n",
    "    \n",
    "    for i in range(50):\n",
    "        current_seq_one_hot = tf.keras.utils.to_categorical(current_seq, num_classes=vocab_size+1)\n",
    "        \n",
    "        data_input = np.reshape(current_seq_one_hot,(1,\n",
    "                                                     current_seq_one_hot.shape[0],\n",
    "                                                     current_seq_one_hot.shape[1]))\n",
    "        \n",
    "        #Get the char int with maximum probability\n",
    "        predicted_char_int = np.argmax(model.predict(data_input)[0])\n",
    "        \n",
    "        if (predicted_char_int != 0):\n",
    "            \n",
    "            #Add to the predicted out, convert int to char\n",
    "            predicted_output = predicted_output + int_to_char[predicted_char_int]\n",
    "        \n",
    "        #Update seq with new value at the end\n",
    "        current_seq = np.roll(current_seq, -1)\n",
    "        current_seq[current_seq.shape[0]-1] = [predicted_char_int]\n",
    "    \n",
    "    print(predicted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlmHfhSUqZnW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VW72ruTt1I9r",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Create a LabdaCallback to do prediction at end of every epoch\n",
    "lambda_checkpoint = tf.keras.callbacks.LambdaCallback(on_epoch_end=predict_seq)\n",
    "\n",
    "#Create a model checkpoint to store model after each epoch if loss reduces\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('char_rnn.h5',\n",
    "                                                      monitor='loss',\n",
    "                                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HAVH-aqyqZna",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "351/352 [============================>.] - ETA: 0s - loss: 2.6616\n",
      "\n",
      "Output sequence after epoch  0  :\n",
      " ansto sou the her ansere to the her ansere to the\n",
      "352/352 [==============================] - 172s 489ms/step - loss: 2.6601\n",
      "Epoch 2/10\n",
      "351/352 [============================>.] - ETA: 0s - loss: 1.6164\n",
      "\n",
      "Output sequence after epoch  1  :\n",
      "leassoo\n",
      "\n",
      "ol  anee the teetentton to mand as astere\n",
      "352/352 [==============================] - 169s 482ms/step - loss: 1.6147\n",
      "Epoch 3/10\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.5572\n",
      "\n",
      "Output sequence after epoch  2  :\n",
      "llasssowd heccrr moffrilly teethen\n",
      "\n",
      "iof betennne l\n",
      "352/352 [==============================] - 169s 481ms/step - loss: 0.5562\n",
      "Epoch 4/10\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.1607\n",
      "\n",
      "Output sequence after epoch  3  :\n",
      "lassatthe who  ho  her eleenentooneaccallanntthe w\n",
      "352/352 [==============================] - 170s 482ms/step - loss: 0.1605\n",
      "Epoch 5/10\n",
      " 39/352 [==>...........................] - ETA: 2:27 - loss: 0.0628"
     ]
    }
   ],
   "source": [
    "batch_size = 2000\n",
    "train_generator = batch_generator(batch_size=batch_size)\n",
    "\n",
    "#Fit generator\n",
    "model.fit_generator(train_generator,\n",
    "                    epochs=10,\n",
    "                    steps_per_epoch = (len(book_num)- sequence_length)// batch_size,                    \n",
    "                    callbacks=[model_checkpoint, lambda_checkpoint])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "2a. CHAR_RNN_BOOK.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
