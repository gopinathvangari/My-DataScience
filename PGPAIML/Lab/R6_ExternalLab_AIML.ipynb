{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_ExternalLab_AIML.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "YYk8NG3yOIT9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### A MNIST-like fashion product database\n",
        "\n",
        "In this, we classify the images into respective classes given in the dataset. We use a Neural Net and a Deep Neural Net in Keras to solve this and check the accuracy scores."
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "tFO6PuxzOIT_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "efNjNImfOIUC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.set_random_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l9C4aAIGOIUH",
        "colab_type": "code",
        "outputId": "b359c287-871d-42dc-a41b-513745921967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "HcoZBStrOIUQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Collect Data"
      ]
    },
    {
      "metadata": {
        "id": "XA1WsFSeOIUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4161a948-c3f5-4f2b-fbef-a73fd1d1dee0"
      },
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "qnbx7TyQOIUY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(trainX, trainY), (testX, testY) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "id": "UbiHj5YPOIUc",
        "colab_type": "code",
        "outputId": "0de6d127-83c6-478d-8ccf-d73bea554ced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(testY[0:5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0fmv7H1ouyg3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainYo = trainY"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "lDAYzkwyOIUj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert both training and testing labels into one-hot vectors.\n",
        "\n",
        "**Hint:** check **tf.keras.utils.to_categorical()**"
      ]
    },
    {
      "metadata": {
        "id": "vBlfYlANOIUk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainY = tf.keras.utils.to_categorical(trainY, num_classes=10)\n",
        "testY = tf.keras.utils.to_categorical(testY, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "slideshow": {
          "slide_type": "fragment"
        },
        "id": "RHV3b9mzOIUq",
        "colab_type": "code",
        "outputId": "a657e3b9-f005-4c9d-ee42-7761d682ff07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "print(trainY.shape)\n",
        "print('First 5 examples now are: ', trainY[0:5])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n",
            "('First 5 examples now are: ', array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FwhQ8e7VOIUw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the data\n",
        "\n",
        "Plot first 10 images in the triaining set and their labels."
      ]
    },
    {
      "metadata": {
        "id": "d2jPKeS50Xuq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "d02ebb98-b5ac-435d-be3d-357fe471563d"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f, ax = plt.subplots(1, 10, sharey=True,figsize=(10,20))\n",
        "\n",
        "for i in range(0,10):\n",
        "  ax[i].imshow(trainX[i],cmap='gray')\n",
        "  ax[i].axis('off')\n",
        "  \n",
        "f.frameon=False\n",
        "plt.show()\n",
        "\n",
        "print('label for each of the above image: ')\n",
        "\n",
        "for i in range(0,10):\n",
        "  print(trainYo[i]),"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAABRCAYAAAAdIZjJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x1440 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "label for each of the above image: \n",
            "9 0 0 3 0 2 7 2 5 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "l4TbJGeSOIU4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build a neural Network with a cross entropy loss function and sgd optimizer in Keras. The output layer with 10 neurons as we have 10 classes."
      ]
    },
    {
      "metadata": {
        "id": "Ac06XZZTOIU6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de754486-23ac-437f-b540-cdf1aabec256"
      },
      "cell_type": "code",
      "source": [
        "trainX.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "PI2-rnhSwpwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5718ab9e-1ce0-4d63-a5a4-58552025721f"
      },
      "cell_type": "code",
      "source": [
        "#Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "#Compile the model\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "3hQpLv3aOIU_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Execute the model using model.fit()"
      ]
    },
    {
      "metadata": {
        "id": "O59C_-IgOIVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3481
        },
        "outputId": "e8f84421-7be6-4c29-ab63-de4851b3f553"
      },
      "cell_type": "code",
      "source": [
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=100,\n",
        "          batch_size=trainX.shape[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 1s 9us/sample - loss: 15.1255 - acc: 0.0538 - val_loss: 14.2723 - val_acc: 0.1125\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 14.2678 - acc: 0.1122 - val_loss: 13.8040 - val_acc: 0.1393\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 13.8326 - acc: 0.1369 - val_loss: 12.9301 - val_acc: 0.1923\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.9670 - acc: 0.1902 - val_loss: 12.8455 - val_acc: 0.1979\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.8804 - acc: 0.1955 - val_loss: 12.8711 - val_acc: 0.1969\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.8796 - acc: 0.1964 - val_loss: 13.3095 - val_acc: 0.1695\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 13.3249 - acc: 0.1682 - val_loss: 13.1045 - val_acc: 0.1850\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 13.1300 - acc: 0.1828 - val_loss: 12.9665 - val_acc: 0.1926\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.9449 - acc: 0.1941 - val_loss: 12.7770 - val_acc: 0.2032\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.7719 - acc: 0.2034 - val_loss: 12.8158 - val_acc: 0.2016\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.8311 - acc: 0.2008 - val_loss: 12.8247 - val_acc: 0.2007\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.8192 - acc: 0.2003 - val_loss: 12.8356 - val_acc: 0.2007\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.8565 - acc: 0.1994 - val_loss: 12.7209 - val_acc: 0.2076\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.7053 - acc: 0.2075 - val_loss: 12.7820 - val_acc: 0.2048\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.7887 - acc: 0.2033 - val_loss: 12.7008 - val_acc: 0.2083\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.6745 - acc: 0.2095 - val_loss: 12.8029 - val_acc: 0.2027\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.8171 - acc: 0.2017 - val_loss: 12.6187 - val_acc: 0.2141\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.5890 - acc: 0.2153 - val_loss: 12.6339 - val_acc: 0.2131\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.6138 - acc: 0.2139 - val_loss: 12.6789 - val_acc: 0.2096\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.6529 - acc: 0.2105 - val_loss: 12.7947 - val_acc: 0.2033\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.8026 - acc: 0.2023 - val_loss: 12.5916 - val_acc: 0.2155\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.5497 - acc: 0.2176 - val_loss: 12.5527 - val_acc: 0.2178\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.5204 - acc: 0.2193 - val_loss: 12.6544 - val_acc: 0.2103\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.6380 - acc: 0.2113 - val_loss: 12.8525 - val_acc: 0.1994\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.8529 - acc: 0.1998 - val_loss: 12.5438 - val_acc: 0.2184\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.5383 - acc: 0.2190 - val_loss: 12.4951 - val_acc: 0.2197\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.4761 - acc: 0.2215 - val_loss: 12.6615 - val_acc: 0.2113\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.6446 - acc: 0.2122 - val_loss: 12.3790 - val_acc: 0.2289\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.3392 - acc: 0.2305 - val_loss: 12.3377 - val_acc: 0.2301\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.2820 - acc: 0.2336 - val_loss: 12.2953 - val_acc: 0.2328\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.2443 - acc: 0.2364 - val_loss: 12.2740 - val_acc: 0.2337\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.2025 - acc: 0.2378 - val_loss: 11.7162 - val_acc: 0.2665\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7144 - acc: 0.2664 - val_loss: 13.8062 - val_acc: 0.1398\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 13.8269 - acc: 0.1389 - val_loss: 12.6307 - val_acc: 0.2119\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.5970 - acc: 0.2157 - val_loss: 12.3172 - val_acc: 0.2321\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.2822 - acc: 0.2337 - val_loss: 12.3270 - val_acc: 0.2320\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.2874 - acc: 0.2344 - val_loss: 12.4374 - val_acc: 0.2228\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.4143 - acc: 0.2244 - val_loss: 12.8751 - val_acc: 0.1983\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.8598 - acc: 0.2000 - val_loss: 12.5519 - val_acc: 0.2187\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.5336 - acc: 0.2203 - val_loss: 12.2767 - val_acc: 0.2359\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.2530 - acc: 0.2372 - val_loss: 12.0109 - val_acc: 0.2519\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.9605 - acc: 0.2545 - val_loss: 11.9563 - val_acc: 0.2551\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.9094 - acc: 0.2583 - val_loss: 11.9462 - val_acc: 0.2545\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.9030 - acc: 0.2573 - val_loss: 12.0085 - val_acc: 0.2525\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.9719 - acc: 0.2547 - val_loss: 11.8989 - val_acc: 0.2580\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.8737 - acc: 0.2596 - val_loss: 12.0876 - val_acc: 0.2475\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.0567 - acc: 0.2495 - val_loss: 11.8084 - val_acc: 0.2641\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7871 - acc: 0.2658 - val_loss: 11.7776 - val_acc: 0.2664\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7698 - acc: 0.2671 - val_loss: 11.7725 - val_acc: 0.2661\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7603 - acc: 0.2676 - val_loss: 11.7621 - val_acc: 0.2677\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7546 - acc: 0.2681 - val_loss: 11.7911 - val_acc: 0.2645\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7819 - acc: 0.2656 - val_loss: 11.8504 - val_acc: 0.2624\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.8476 - acc: 0.2624 - val_loss: 11.8555 - val_acc: 0.2605\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.8382 - acc: 0.2619 - val_loss: 12.1265 - val_acc: 0.2457\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 12.0917 - acc: 0.2474 - val_loss: 11.7249 - val_acc: 0.2704\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7233 - acc: 0.2702 - val_loss: 11.7483 - val_acc: 0.2682\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7400 - acc: 0.2689 - val_loss: 11.7988 - val_acc: 0.2659\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7834 - acc: 0.2667 - val_loss: 11.7143 - val_acc: 0.2708\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7107 - acc: 0.2705 - val_loss: 11.7091 - val_acc: 0.2713\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7056 - acc: 0.2715 - val_loss: 11.6808 - val_acc: 0.2729\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6863 - acc: 0.2721 - val_loss: 11.6920 - val_acc: 0.2729\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6944 - acc: 0.2723 - val_loss: 11.6837 - val_acc: 0.2724\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6835 - acc: 0.2725 - val_loss: 11.6984 - val_acc: 0.2720\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6992 - acc: 0.2720 - val_loss: 11.7522 - val_acc: 0.2672\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7403 - acc: 0.2687 - val_loss: 11.8982 - val_acc: 0.2594\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.8984 - acc: 0.2593 - val_loss: 11.6556 - val_acc: 0.2738\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6590 - acc: 0.2743 - val_loss: 11.6584 - val_acc: 0.2746\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6574 - acc: 0.2749 - val_loss: 11.6430 - val_acc: 0.2748\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6456 - acc: 0.2751 - val_loss: 11.6547 - val_acc: 0.2752\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6529 - acc: 0.2749 - val_loss: 11.6622 - val_acc: 0.2738\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6630 - acc: 0.2738 - val_loss: 11.7403 - val_acc: 0.2691\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7490 - acc: 0.2690 - val_loss: 11.6681 - val_acc: 0.2722\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6731 - acc: 0.2730 - val_loss: 11.8187 - val_acc: 0.2648\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.8165 - acc: 0.2649 - val_loss: 11.6301 - val_acc: 0.2761\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6257 - acc: 0.2765 - val_loss: 11.6311 - val_acc: 0.2764\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6277 - acc: 0.2765 - val_loss: 11.6181 - val_acc: 0.2769\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6197 - acc: 0.2768 - val_loss: 11.6297 - val_acc: 0.2764\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6324 - acc: 0.2763 - val_loss: 11.6614 - val_acc: 0.2736\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6671 - acc: 0.2734 - val_loss: 11.8030 - val_acc: 0.2655\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.8041 - acc: 0.2656 - val_loss: 11.6110 - val_acc: 0.2781\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6085 - acc: 0.2776 - val_loss: 11.6088 - val_acc: 0.2780\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6043 - acc: 0.2779 - val_loss: 11.6033 - val_acc: 0.2777\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6021 - acc: 0.2782 - val_loss: 11.6012 - val_acc: 0.2782\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.5980 - acc: 0.2785 - val_loss: 11.5964 - val_acc: 0.2783\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.5948 - acc: 0.2787 - val_loss: 11.6009 - val_acc: 0.2785\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.5944 - acc: 0.2788 - val_loss: 11.6044 - val_acc: 0.2781\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6026 - acc: 0.2779 - val_loss: 11.6394 - val_acc: 0.2756\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6428 - acc: 0.2758 - val_loss: 11.6771 - val_acc: 0.2729\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6760 - acc: 0.2725 - val_loss: 11.8835 - val_acc: 0.2601\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.8760 - acc: 0.2610 - val_loss: 11.5943 - val_acc: 0.2786\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.5855 - acc: 0.2793 - val_loss: 11.5824 - val_acc: 0.2800\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.5833 - acc: 0.2796 - val_loss: 11.5880 - val_acc: 0.2798\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.5831 - acc: 0.2795 - val_loss: 11.5832 - val_acc: 0.2792\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.5820 - acc: 0.2795 - val_loss: 11.5866 - val_acc: 0.2797\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.5805 - acc: 0.2795 - val_loss: 11.6037 - val_acc: 0.2781\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6028 - acc: 0.2781 - val_loss: 11.6663 - val_acc: 0.2744\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6689 - acc: 0.2741 - val_loss: 11.6139 - val_acc: 0.2772\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.6158 - acc: 0.2769 - val_loss: 11.7503 - val_acc: 0.2693\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.7562 - acc: 0.2689 - val_loss: 11.5746 - val_acc: 0.2806\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 11.5711 - acc: 0.2804 - val_loss: 11.5751 - val_acc: 0.2807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb89248dd10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "JdzDtGwDOIVF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### In the above Neural Network model add Batch Normalization layer after the input layer and repeat the steps."
      ]
    },
    {
      "metadata": {
        "id": "kndfpdidOIVI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Initialize Sequential model\n",
        "model1 = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model1.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "#Normalize the data\n",
        "model1.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model1.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "#Compile the model\n",
        "model1.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "mwk3T5LJOIVN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Execute the model"
      ]
    },
    {
      "metadata": {
        "id": "JNLR8tcBOIVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3410
        },
        "outputId": "65d7d8b4-f9fa-4bff-ecb6-a7af3e964e24"
      },
      "cell_type": "code",
      "source": [
        "model1.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=100,\n",
        "          batch_size=trainX.shape[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 0s 6us/sample - loss: 3.2063 - acc: 0.0718 - val_loss: 13.6401 - val_acc: 0.0719\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.8699 - acc: 0.0948 - val_loss: 11.7936 - val_acc: 0.0741\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.5931 - acc: 0.1238 - val_loss: 9.4453 - val_acc: 0.0889\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.3649 - acc: 0.1635 - val_loss: 7.2989 - val_acc: 0.1216\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.1761 - acc: 0.2094 - val_loss: 5.7400 - val_acc: 0.1694\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.0192 - acc: 0.2611 - val_loss: 4.7099 - val_acc: 0.2218\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.8883 - acc: 0.3122 - val_loss: 4.0293 - val_acc: 0.2716\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.7787 - acc: 0.3559 - val_loss: 3.5666 - val_acc: 0.3149\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.6862 - acc: 0.3927 - val_loss: 3.2365 - val_acc: 0.3522\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.6076 - acc: 0.4254 - val_loss: 2.9902 - val_acc: 0.3822\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.5402 - acc: 0.4534 - val_loss: 2.7979 - val_acc: 0.4100\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.4819 - acc: 0.4764 - val_loss: 2.6426 - val_acc: 0.4356\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.4310 - acc: 0.4964 - val_loss: 2.5129 - val_acc: 0.4562\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.3863 - acc: 0.5139 - val_loss: 2.4017 - val_acc: 0.4726\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.3467 - acc: 0.5302 - val_loss: 2.3044 - val_acc: 0.4874\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.3114 - acc: 0.5444 - val_loss: 2.2181 - val_acc: 0.5022\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.2797 - acc: 0.5575 - val_loss: 2.1407 - val_acc: 0.5145\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.2511 - acc: 0.5694 - val_loss: 2.0706 - val_acc: 0.5266\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.2252 - acc: 0.5793 - val_loss: 2.0065 - val_acc: 0.5362\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.2016 - acc: 0.5882 - val_loss: 1.9476 - val_acc: 0.5457\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.1800 - acc: 0.5966 - val_loss: 1.8934 - val_acc: 0.5544\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.1602 - acc: 0.6040 - val_loss: 1.8430 - val_acc: 0.5613\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.1418 - acc: 0.6104 - val_loss: 1.7962 - val_acc: 0.5686\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.1248 - acc: 0.6162 - val_loss: 1.7525 - val_acc: 0.5749\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.1090 - acc: 0.6218 - val_loss: 1.7116 - val_acc: 0.5803\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0943 - acc: 0.6266 - val_loss: 1.6732 - val_acc: 0.5857\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0804 - acc: 0.6316 - val_loss: 1.6370 - val_acc: 0.5896\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0674 - acc: 0.6361 - val_loss: 1.6029 - val_acc: 0.5940\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0552 - acc: 0.6401 - val_loss: 1.5707 - val_acc: 0.5981\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0436 - acc: 0.6439 - val_loss: 1.5402 - val_acc: 0.6032\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0327 - acc: 0.6474 - val_loss: 1.5114 - val_acc: 0.6072\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0223 - acc: 0.6512 - val_loss: 1.4840 - val_acc: 0.6093\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0125 - acc: 0.6546 - val_loss: 1.4579 - val_acc: 0.6129\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0031 - acc: 0.6574 - val_loss: 1.4331 - val_acc: 0.6158\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9941 - acc: 0.6606 - val_loss: 1.4095 - val_acc: 0.6191\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9855 - acc: 0.6633 - val_loss: 1.3870 - val_acc: 0.6217\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9773 - acc: 0.6661 - val_loss: 1.3655 - val_acc: 0.6249\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9695 - acc: 0.6687 - val_loss: 1.3449 - val_acc: 0.6277\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9619 - acc: 0.6711 - val_loss: 1.3252 - val_acc: 0.6304\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9547 - acc: 0.6734 - val_loss: 1.3062 - val_acc: 0.6337\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9477 - acc: 0.6761 - val_loss: 1.2881 - val_acc: 0.6351\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9410 - acc: 0.6780 - val_loss: 1.2707 - val_acc: 0.6385\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9345 - acc: 0.6800 - val_loss: 1.2539 - val_acc: 0.6409\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9283 - acc: 0.6820 - val_loss: 1.2378 - val_acc: 0.6429\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9222 - acc: 0.6840 - val_loss: 1.2224 - val_acc: 0.6452\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9164 - acc: 0.6854 - val_loss: 1.2075 - val_acc: 0.6470\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9107 - acc: 0.6872 - val_loss: 1.1932 - val_acc: 0.6493\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9053 - acc: 0.6893 - val_loss: 1.1793 - val_acc: 0.6510\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9000 - acc: 0.6911 - val_loss: 1.1660 - val_acc: 0.6526\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8948 - acc: 0.6925 - val_loss: 1.1532 - val_acc: 0.6548\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8898 - acc: 0.6941 - val_loss: 1.1407 - val_acc: 0.6565\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8850 - acc: 0.6958 - val_loss: 1.1287 - val_acc: 0.6592\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8803 - acc: 0.6971 - val_loss: 1.1171 - val_acc: 0.6616\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8757 - acc: 0.6987 - val_loss: 1.1059 - val_acc: 0.6632\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 0.8713 - acc: 0.7002 - val_loss: 1.0951 - val_acc: 0.6652\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8669 - acc: 0.7018 - val_loss: 1.0846 - val_acc: 0.6665\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8627 - acc: 0.7031 - val_loss: 1.0744 - val_acc: 0.6682\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8586 - acc: 0.7043 - val_loss: 1.0645 - val_acc: 0.6703\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8546 - acc: 0.7054 - val_loss: 1.0550 - val_acc: 0.6726\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8507 - acc: 0.7067 - val_loss: 1.0457 - val_acc: 0.6742\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8469 - acc: 0.7077 - val_loss: 1.0367 - val_acc: 0.6759\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8432 - acc: 0.7090 - val_loss: 1.0280 - val_acc: 0.6770\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8395 - acc: 0.7101 - val_loss: 1.0195 - val_acc: 0.6791\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8360 - acc: 0.7111 - val_loss: 1.0113 - val_acc: 0.6809\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8325 - acc: 0.7124 - val_loss: 1.0033 - val_acc: 0.6832\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8291 - acc: 0.7131 - val_loss: 0.9955 - val_acc: 0.6842\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8258 - acc: 0.7141 - val_loss: 0.9880 - val_acc: 0.6862\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8226 - acc: 0.7153 - val_loss: 0.9806 - val_acc: 0.6873\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8194 - acc: 0.7161 - val_loss: 0.9735 - val_acc: 0.6885\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8163 - acc: 0.7173 - val_loss: 0.9665 - val_acc: 0.6900\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8132 - acc: 0.7185 - val_loss: 0.9597 - val_acc: 0.6908\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8103 - acc: 0.7195 - val_loss: 0.9531 - val_acc: 0.6913\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8073 - acc: 0.7205 - val_loss: 0.9467 - val_acc: 0.6918\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8045 - acc: 0.7216 - val_loss: 0.9405 - val_acc: 0.6932\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8017 - acc: 0.7225 - val_loss: 0.9344 - val_acc: 0.6947\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7989 - acc: 0.7229 - val_loss: 0.9284 - val_acc: 0.6961\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7962 - acc: 0.7238 - val_loss: 0.9227 - val_acc: 0.6972\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7936 - acc: 0.7246 - val_loss: 0.9170 - val_acc: 0.6979\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7910 - acc: 0.7254 - val_loss: 0.9115 - val_acc: 0.6989\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7885 - acc: 0.7263 - val_loss: 0.9061 - val_acc: 0.7003\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7860 - acc: 0.7271 - val_loss: 0.9009 - val_acc: 0.7012\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7835 - acc: 0.7280 - val_loss: 0.8958 - val_acc: 0.7022\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7811 - acc: 0.7288 - val_loss: 0.8908 - val_acc: 0.7038\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7787 - acc: 0.7297 - val_loss: 0.8859 - val_acc: 0.7050\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7764 - acc: 0.7307 - val_loss: 0.8812 - val_acc: 0.7064\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7741 - acc: 0.7315 - val_loss: 0.8766 - val_acc: 0.7073\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7719 - acc: 0.7323 - val_loss: 0.8720 - val_acc: 0.7082\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7697 - acc: 0.7330 - val_loss: 0.8676 - val_acc: 0.7094\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7675 - acc: 0.7338 - val_loss: 0.8633 - val_acc: 0.7111\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7653 - acc: 0.7343 - val_loss: 0.8590 - val_acc: 0.7117\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 0.7632 - acc: 0.7350 - val_loss: 0.8549 - val_acc: 0.7133\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7612 - acc: 0.7359 - val_loss: 0.8509 - val_acc: 0.7140\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7591 - acc: 0.7367 - val_loss: 0.8469 - val_acc: 0.7153\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7571 - acc: 0.7376 - val_loss: 0.8431 - val_acc: 0.7169\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7552 - acc: 0.7381 - val_loss: 0.8393 - val_acc: 0.7179\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7532 - acc: 0.7387 - val_loss: 0.8356 - val_acc: 0.7191\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7513 - acc: 0.7394 - val_loss: 0.8320 - val_acc: 0.7202\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7494 - acc: 0.7400 - val_loss: 0.8284 - val_acc: 0.7209\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7476 - acc: 0.7405 - val_loss: 0.8250 - val_acc: 0.7225\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7457 - acc: 0.7412 - val_loss: 0.8216 - val_acc: 0.7227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb88fe05410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "Py-KwkmjOIVU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Customize the learning rate to 0.001 in sgd optimizer and run the model"
      ]
    },
    {
      "metadata": {
        "id": "yLXUE9jWOIVV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.001)\n",
        "#Initialize Sequential model\n",
        "model2 = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model2.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "#Normalize the data\n",
        "model2.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model2.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "#Compile the model\n",
        "model2.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pJUqA5T4OIVc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3410
        },
        "outputId": "bb37acd7-f208-4a45-c6af-29c19ecc6d88"
      },
      "cell_type": "code",
      "source": [
        "model2.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=100,\n",
        "          batch_size=trainX.shape[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 0s 7us/sample - loss: 2.8004 - acc: 0.0892 - val_loss: 11.0249 - val_acc: 0.1357\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.5320 - acc: 0.1309 - val_loss: 9.1372 - val_acc: 0.1633\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.3169 - acc: 0.1890 - val_loss: 7.7145 - val_acc: 0.1925\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.1440 - acc: 0.2485 - val_loss: 6.5866 - val_acc: 0.2218\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.0036 - acc: 0.3002 - val_loss: 5.6968 - val_acc: 0.2508\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.8877 - acc: 0.3435 - val_loss: 4.9982 - val_acc: 0.2799\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.7909 - acc: 0.3801 - val_loss: 4.4471 - val_acc: 0.3039\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.7089 - acc: 0.4123 - val_loss: 4.0048 - val_acc: 0.3274\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.6385 - acc: 0.4390 - val_loss: 3.6455 - val_acc: 0.3498\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.5775 - acc: 0.4612 - val_loss: 3.3503 - val_acc: 0.3709\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.5240 - acc: 0.4803 - val_loss: 3.1053 - val_acc: 0.3931\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.4768 - acc: 0.4983 - val_loss: 2.8993 - val_acc: 0.4132\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.4348 - acc: 0.5151 - val_loss: 2.7241 - val_acc: 0.4302\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.3970 - acc: 0.5293 - val_loss: 2.5737 - val_acc: 0.4470\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.3630 - acc: 0.5413 - val_loss: 2.4433 - val_acc: 0.4653\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.3321 - acc: 0.5524 - val_loss: 2.3291 - val_acc: 0.4797\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.3039 - acc: 0.5627 - val_loss: 2.2284 - val_acc: 0.4924\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.2780 - acc: 0.5724 - val_loss: 2.1387 - val_acc: 0.5030\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.2541 - acc: 0.5809 - val_loss: 2.0585 - val_acc: 0.5148\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.2321 - acc: 0.5889 - val_loss: 1.9863 - val_acc: 0.5245\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.2116 - acc: 0.5961 - val_loss: 1.9208 - val_acc: 0.5343\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.1925 - acc: 0.6035 - val_loss: 1.8611 - val_acc: 0.5429\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.1746 - acc: 0.6098 - val_loss: 1.8067 - val_acc: 0.5516\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.1579 - acc: 0.6152 - val_loss: 1.7566 - val_acc: 0.5616\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.1422 - acc: 0.6205 - val_loss: 1.7105 - val_acc: 0.5701\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.1274 - acc: 0.6250 - val_loss: 1.6679 - val_acc: 0.5769\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.1134 - acc: 0.6294 - val_loss: 1.6283 - val_acc: 0.5823\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.1002 - acc: 0.6342 - val_loss: 1.5913 - val_acc: 0.5876\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0876 - acc: 0.6385 - val_loss: 1.5568 - val_acc: 0.5924\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0757 - acc: 0.6426 - val_loss: 1.5245 - val_acc: 0.5968\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0643 - acc: 0.6458 - val_loss: 1.4941 - val_acc: 0.6010\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0535 - acc: 0.6492 - val_loss: 1.4654 - val_acc: 0.6046\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0432 - acc: 0.6525 - val_loss: 1.4384 - val_acc: 0.6088\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0333 - acc: 0.6556 - val_loss: 1.4128 - val_acc: 0.6129\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0238 - acc: 0.6584 - val_loss: 1.3885 - val_acc: 0.6164\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0148 - acc: 0.6614 - val_loss: 1.3655 - val_acc: 0.6205\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 1.0061 - acc: 0.6642 - val_loss: 1.3436 - val_acc: 0.6233\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9977 - acc: 0.6672 - val_loss: 1.3228 - val_acc: 0.6276\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9896 - acc: 0.6697 - val_loss: 1.3030 - val_acc: 0.6306\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9819 - acc: 0.6723 - val_loss: 1.2840 - val_acc: 0.6332\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9744 - acc: 0.6747 - val_loss: 1.2659 - val_acc: 0.6351\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9672 - acc: 0.6769 - val_loss: 1.2486 - val_acc: 0.6375\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9603 - acc: 0.6787 - val_loss: 1.2320 - val_acc: 0.6404\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9535 - acc: 0.6806 - val_loss: 1.2161 - val_acc: 0.6425\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9470 - acc: 0.6825 - val_loss: 1.2008 - val_acc: 0.6452\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9407 - acc: 0.6846 - val_loss: 1.1861 - val_acc: 0.6466\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9346 - acc: 0.6861 - val_loss: 1.1720 - val_acc: 0.6486\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9287 - acc: 0.6877 - val_loss: 1.1585 - val_acc: 0.6514\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9230 - acc: 0.6894 - val_loss: 1.1454 - val_acc: 0.6527\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9174 - acc: 0.6908 - val_loss: 1.1328 - val_acc: 0.6545\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9120 - acc: 0.6923 - val_loss: 1.1206 - val_acc: 0.6562\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9068 - acc: 0.6940 - val_loss: 1.1089 - val_acc: 0.6585\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.9017 - acc: 0.6955 - val_loss: 1.0976 - val_acc: 0.6609\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8967 - acc: 0.6966 - val_loss: 1.0866 - val_acc: 0.6625\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8919 - acc: 0.6980 - val_loss: 1.0761 - val_acc: 0.6656\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8872 - acc: 0.6992 - val_loss: 1.0658 - val_acc: 0.6669\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8826 - acc: 0.7008 - val_loss: 1.0559 - val_acc: 0.6688\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8781 - acc: 0.7022 - val_loss: 1.0463 - val_acc: 0.6705\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8738 - acc: 0.7039 - val_loss: 1.0370 - val_acc: 0.6726\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8695 - acc: 0.7052 - val_loss: 1.0280 - val_acc: 0.6752\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8654 - acc: 0.7066 - val_loss: 1.0193 - val_acc: 0.6770\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8613 - acc: 0.7079 - val_loss: 1.0108 - val_acc: 0.6790\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8574 - acc: 0.7090 - val_loss: 1.0026 - val_acc: 0.6805\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8535 - acc: 0.7105 - val_loss: 0.9946 - val_acc: 0.6814\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8498 - acc: 0.7117 - val_loss: 0.9869 - val_acc: 0.6831\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8461 - acc: 0.7129 - val_loss: 0.9794 - val_acc: 0.6847\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8425 - acc: 0.7137 - val_loss: 0.9720 - val_acc: 0.6855\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8389 - acc: 0.7148 - val_loss: 0.9649 - val_acc: 0.6868\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8355 - acc: 0.7159 - val_loss: 0.9580 - val_acc: 0.6887\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8321 - acc: 0.7168 - val_loss: 0.9513 - val_acc: 0.6900\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8288 - acc: 0.7179 - val_loss: 0.9447 - val_acc: 0.6911\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8256 - acc: 0.7189 - val_loss: 0.9384 - val_acc: 0.6927\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8224 - acc: 0.7199 - val_loss: 0.9321 - val_acc: 0.6934\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8193 - acc: 0.7210 - val_loss: 0.9261 - val_acc: 0.6938\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8162 - acc: 0.7220 - val_loss: 0.9202 - val_acc: 0.6950\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8133 - acc: 0.7231 - val_loss: 0.9145 - val_acc: 0.6968\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8103 - acc: 0.7243 - val_loss: 0.9089 - val_acc: 0.6990\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8074 - acc: 0.7252 - val_loss: 0.9034 - val_acc: 0.7010\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8046 - acc: 0.7259 - val_loss: 0.8981 - val_acc: 0.7025\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.8019 - acc: 0.7268 - val_loss: 0.8929 - val_acc: 0.7040\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7991 - acc: 0.7276 - val_loss: 0.8878 - val_acc: 0.7045\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7965 - acc: 0.7282 - val_loss: 0.8829 - val_acc: 0.7053\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7939 - acc: 0.7290 - val_loss: 0.8781 - val_acc: 0.7062\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7913 - acc: 0.7300 - val_loss: 0.8734 - val_acc: 0.7077\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7888 - acc: 0.7308 - val_loss: 0.8688 - val_acc: 0.7094\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7863 - acc: 0.7319 - val_loss: 0.8643 - val_acc: 0.7108\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7838 - acc: 0.7327 - val_loss: 0.8599 - val_acc: 0.7120\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7814 - acc: 0.7332 - val_loss: 0.8556 - val_acc: 0.7132\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7791 - acc: 0.7342 - val_loss: 0.8514 - val_acc: 0.7141\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7767 - acc: 0.7352 - val_loss: 0.8473 - val_acc: 0.7149\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7745 - acc: 0.7360 - val_loss: 0.8433 - val_acc: 0.7161\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7722 - acc: 0.7368 - val_loss: 0.8394 - val_acc: 0.7179\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7700 - acc: 0.7372 - val_loss: 0.8356 - val_acc: 0.7196\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7678 - acc: 0.7378 - val_loss: 0.8319 - val_acc: 0.7204\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7657 - acc: 0.7387 - val_loss: 0.8282 - val_acc: 0.7217\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7636 - acc: 0.7393 - val_loss: 0.8246 - val_acc: 0.7227\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7615 - acc: 0.7401 - val_loss: 0.8211 - val_acc: 0.7238\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7595 - acc: 0.7408 - val_loss: 0.8177 - val_acc: 0.7255\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7575 - acc: 0.7416 - val_loss: 0.8143 - val_acc: 0.7266\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 0.7555 - acc: 0.7423 - val_loss: 0.8110 - val_acc: 0.7277\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb88fe05250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "j9CSqKvpOIVk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build the Neural Network model with 3 Dense layers with 100,100,10 neurons respectively in each layer. Use cross entropy loss function and singmoid as activation in the hidden layers and softmax as activation function in the output layer. Use sgd optimizer with learning rate 0.03."
      ]
    },
    {
      "metadata": {
        "id": "GGAad54JOIVm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Initialize Sequential model\n",
        "model3 = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model3.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "#Normalize the data\n",
        "model3.add(tf.keras.layers.BatchNormalization())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MQ7oIymROIVp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Add 1st hidden layer\n",
        "model3.add(tf.keras.layers.Dense(100, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X-O-fFxnOIVt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Add 2nd hidden layer\n",
        "model3.add(tf.keras.layers.Dense(100, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BiP7IL52OIVw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Add OUTPUT layer\n",
        "model3.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZOfC4Urwyt9U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Create optimizer with non-default learning rate\n",
        "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.03)\n",
        "\n",
        "#Compile the model\n",
        "model3.compile(optimizer=sgd_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "Nr2YsZV0OIV0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Review model"
      ]
    },
    {
      "metadata": {
        "id": "h4ojW6-oOIV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "6aaed305-66ca-40c6-b25f-ef7b7b957e4b"
      },
      "cell_type": "code",
      "source": [
        "model3.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_3 (Reshape)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_2 (Ba (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 92,746\n",
            "Trainable params: 91,178\n",
            "Non-trainable params: 1,568\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "id": "gfFGmbZLOIV5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Run the model"
      ]
    },
    {
      "metadata": {
        "id": "bIkbMEN5OIV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3410
        },
        "outputId": "eddc1167-a1b4-4fb8-f668-c9fb0243b25c"
      },
      "cell_type": "code",
      "source": [
        "model3.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=100,\n",
        "          batch_size=trainX.shape[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 0s 8us/sample - loss: 2.5523 - acc: 0.0879 - val_loss: 2.4930 - val_acc: 0.0980\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.4905 - acc: 0.0834 - val_loss: 2.4439 - val_acc: 0.0976\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.4427 - acc: 0.0788 - val_loss: 2.4052 - val_acc: 0.0976\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.4052 - acc: 0.0759 - val_loss: 2.3743 - val_acc: 0.0996\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.3753 - acc: 0.0742 - val_loss: 2.3493 - val_acc: 0.0998\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.3510 - acc: 0.0736 - val_loss: 2.3287 - val_acc: 0.1000\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.3311 - acc: 0.0730 - val_loss: 2.3115 - val_acc: 0.1008\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.3145 - acc: 0.0737 - val_loss: 2.2970 - val_acc: 0.1014\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.3005 - acc: 0.0745 - val_loss: 2.2847 - val_acc: 0.1014\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2885 - acc: 0.0759 - val_loss: 2.2740 - val_acc: 0.1029\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2781 - acc: 0.0777 - val_loss: 2.2646 - val_acc: 0.1038\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2690 - acc: 0.0810 - val_loss: 2.2564 - val_acc: 0.1060\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2610 - acc: 0.0875 - val_loss: 2.2490 - val_acc: 0.1087\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2538 - acc: 0.0993 - val_loss: 2.2424 - val_acc: 0.1123\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2473 - acc: 0.1145 - val_loss: 2.2363 - val_acc: 0.1177\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2414 - acc: 0.1330 - val_loss: 2.2308 - val_acc: 0.1277\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2360 - acc: 0.1522 - val_loss: 2.2256 - val_acc: 0.1398\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2310 - acc: 0.1726 - val_loss: 2.2208 - val_acc: 0.1549\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2263 - acc: 0.1935 - val_loss: 2.2163 - val_acc: 0.1716\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2218 - acc: 0.2195 - val_loss: 2.2120 - val_acc: 0.1935\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2176 - acc: 0.2460 - val_loss: 2.2079 - val_acc: 0.2161\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2136 - acc: 0.2740 - val_loss: 2.2040 - val_acc: 0.2372\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2098 - acc: 0.3009 - val_loss: 2.2002 - val_acc: 0.2598\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2060 - acc: 0.3236 - val_loss: 2.1965 - val_acc: 0.2778\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.2024 - acc: 0.3441 - val_loss: 2.1928 - val_acc: 0.2942\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1989 - acc: 0.3616 - val_loss: 2.1893 - val_acc: 0.3088\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1955 - acc: 0.3769 - val_loss: 2.1859 - val_acc: 0.3209\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1921 - acc: 0.3918 - val_loss: 2.1824 - val_acc: 0.3320\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1887 - acc: 0.4059 - val_loss: 2.1791 - val_acc: 0.3405\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1855 - acc: 0.4181 - val_loss: 2.1757 - val_acc: 0.3496\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1822 - acc: 0.4289 - val_loss: 2.1725 - val_acc: 0.3576\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1790 - acc: 0.4392 - val_loss: 2.1692 - val_acc: 0.3624\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1758 - acc: 0.4481 - val_loss: 2.1659 - val_acc: 0.3702\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1727 - acc: 0.4552 - val_loss: 2.1627 - val_acc: 0.3750\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1696 - acc: 0.4616 - val_loss: 2.1595 - val_acc: 0.3803\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1665 - acc: 0.4679 - val_loss: 2.1563 - val_acc: 0.3856\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1634 - acc: 0.4726 - val_loss: 2.1531 - val_acc: 0.3907\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1603 - acc: 0.4771 - val_loss: 2.1500 - val_acc: 0.3953\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1572 - acc: 0.4820 - val_loss: 2.1468 - val_acc: 0.3992\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1542 - acc: 0.4870 - val_loss: 2.1437 - val_acc: 0.4035\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1511 - acc: 0.4909 - val_loss: 2.1405 - val_acc: 0.4062\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1481 - acc: 0.4948 - val_loss: 2.1374 - val_acc: 0.4115\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1450 - acc: 0.4982 - val_loss: 2.1343 - val_acc: 0.4163\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1420 - acc: 0.5019 - val_loss: 2.1312 - val_acc: 0.4193\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1390 - acc: 0.5050 - val_loss: 2.1280 - val_acc: 0.4237\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1360 - acc: 0.5083 - val_loss: 2.1249 - val_acc: 0.4278\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1330 - acc: 0.5116 - val_loss: 2.1218 - val_acc: 0.4318\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1300 - acc: 0.5141 - val_loss: 2.1187 - val_acc: 0.4360\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1270 - acc: 0.5172 - val_loss: 2.1156 - val_acc: 0.4404\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1240 - acc: 0.5203 - val_loss: 2.1125 - val_acc: 0.4436\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1210 - acc: 0.5236 - val_loss: 2.1095 - val_acc: 0.4475\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1181 - acc: 0.5263 - val_loss: 2.1064 - val_acc: 0.4513\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1151 - acc: 0.5285 - val_loss: 2.1033 - val_acc: 0.4547\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1121 - acc: 0.5311 - val_loss: 2.1002 - val_acc: 0.4581\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1092 - acc: 0.5338 - val_loss: 2.0972 - val_acc: 0.4622\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1062 - acc: 0.5361 - val_loss: 2.0941 - val_acc: 0.4667\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1032 - acc: 0.5385 - val_loss: 2.0910 - val_acc: 0.4704\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.1003 - acc: 0.5407 - val_loss: 2.0880 - val_acc: 0.4734\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0973 - acc: 0.5429 - val_loss: 2.0849 - val_acc: 0.4763\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0944 - acc: 0.5449 - val_loss: 2.0819 - val_acc: 0.4795\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0914 - acc: 0.5469 - val_loss: 2.0788 - val_acc: 0.4816\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0885 - acc: 0.5488 - val_loss: 2.0758 - val_acc: 0.4826\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0856 - acc: 0.5509 - val_loss: 2.0727 - val_acc: 0.4864\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0826 - acc: 0.5529 - val_loss: 2.0697 - val_acc: 0.4889\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0797 - acc: 0.5551 - val_loss: 2.0666 - val_acc: 0.4911\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0768 - acc: 0.5570 - val_loss: 2.0636 - val_acc: 0.4943\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0738 - acc: 0.5591 - val_loss: 2.0606 - val_acc: 0.4968\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0709 - acc: 0.5612 - val_loss: 2.0576 - val_acc: 0.4987\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0680 - acc: 0.5628 - val_loss: 2.0545 - val_acc: 0.5025\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0651 - acc: 0.5643 - val_loss: 2.0515 - val_acc: 0.5054\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0622 - acc: 0.5659 - val_loss: 2.0485 - val_acc: 0.5085\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0593 - acc: 0.5674 - val_loss: 2.0455 - val_acc: 0.5115\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0563 - acc: 0.5692 - val_loss: 2.0425 - val_acc: 0.5143\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0534 - acc: 0.5711 - val_loss: 2.0395 - val_acc: 0.5173\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0505 - acc: 0.5729 - val_loss: 2.0365 - val_acc: 0.5197\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0476 - acc: 0.5744 - val_loss: 2.0335 - val_acc: 0.5227\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0447 - acc: 0.5761 - val_loss: 2.0305 - val_acc: 0.5244\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0418 - acc: 0.5778 - val_loss: 2.0275 - val_acc: 0.5266\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0389 - acc: 0.5792 - val_loss: 2.0245 - val_acc: 0.5289\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0360 - acc: 0.5807 - val_loss: 2.0215 - val_acc: 0.5318\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0331 - acc: 0.5823 - val_loss: 2.0185 - val_acc: 0.5333\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0302 - acc: 0.5840 - val_loss: 2.0155 - val_acc: 0.5351\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.0273 - acc: 0.5851 - val_loss: 2.0125 - val_acc: 0.5376\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0244 - acc: 0.5865 - val_loss: 2.0096 - val_acc: 0.5397\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0215 - acc: 0.5875 - val_loss: 2.0066 - val_acc: 0.5417\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.0186 - acc: 0.5890 - val_loss: 2.0036 - val_acc: 0.5445\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.0157 - acc: 0.5901 - val_loss: 2.0006 - val_acc: 0.5463\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.0129 - acc: 0.5912 - val_loss: 1.9977 - val_acc: 0.5483\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0100 - acc: 0.5926 - val_loss: 1.9947 - val_acc: 0.5503\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 0s 3us/sample - loss: 2.0071 - acc: 0.5936 - val_loss: 1.9918 - val_acc: 0.5533\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0042 - acc: 0.5946 - val_loss: 1.9888 - val_acc: 0.5558\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2.0013 - acc: 0.5961 - val_loss: 1.9858 - val_acc: 0.5574\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1.9984 - acc: 0.5972 - val_loss: 1.9829 - val_acc: 0.5597\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1.9956 - acc: 0.5984 - val_loss: 1.9799 - val_acc: 0.5616\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1.9927 - acc: 0.5996 - val_loss: 1.9770 - val_acc: 0.5636\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1.9898 - acc: 0.6006 - val_loss: 1.9741 - val_acc: 0.5657\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1.9869 - acc: 0.6017 - val_loss: 1.9711 - val_acc: 0.5678\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1.9840 - acc: 0.6026 - val_loss: 1.9682 - val_acc: 0.5699\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1.9812 - acc: 0.6034 - val_loss: 1.9652 - val_acc: 0.5711\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1.9783 - acc: 0.6045 - val_loss: 1.9623 - val_acc: 0.5723\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb887513c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}